{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better look in jupyternotebook. Please execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    body {\n",
       "        --vscode-font-family: \"LXGW WenKai\";\n",
       "        line-height: 2; Í\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "    body {\n",
    "        --vscode-font-family: \"LXGW WenKai\";\n",
    "        line-height: 2; Í\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import findspark \n",
    "findspark.init()\n",
    "\n",
    "# for sql\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, sum,avg,max,count\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType, LongType, TimestampType\n",
    "\n",
    "# for time \n",
    "import datetime as dt\n",
    "import psutil\n",
    "\n",
    "# for plot\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren des Schemas basierend auf der Struktur\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"category_id\", LongType(), True),\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"user_session\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFM-Segmentation\n",
    "\n",
    "This section aims at solving a big data use case demonstrated by the RFM-Segmentation.\n",
    "A short introduction to RFM-Analysis and the RFM-Segmentation\n",
    "\n",
    "### RFM Analysis\n",
    "RFM is a method used for analyzing customer value. It is commonly used in database marketing and direct marketing and has received particular attention in retail and professional services industries.\n",
    "\n",
    "RFM stands for the three dimensions:\n",
    "\n",
    "* Recency – How recently did the customer purchase?\n",
    "* Frequency – How often do they purchase?\n",
    "* Monetary Value – How much do they spend?\n",
    "\n",
    "source: [wikipedia](https://en.wikipedia.org/wiki/RFM_(market_research))\n",
    "\n",
    "so we will calculate the 3 attributes Recency, Frequency, and Monetary\n",
    "\n",
    "### RFM Segmentation\n",
    "\n",
    "RFM segmentation is a useful tool for identifying groups of clients who should be given extra attention. RFM segmentation enables marketers to target specific groups of customers with communications that are far more relevant to their unique behaviors, resulting in improved response rates, enhanced loyalty, and increased customer lifetime value. RFM segmentation is a method for segmenting data, just like other approaches.\n",
    "\n",
    "The simplest way to create customers segments from RFM Model is to use Quartiles. We assign a score from 1 to 4 to Recency, Frequency and Monetary. Four is the best/highest value, and one is the lowest/worst value. A final RFM score is calculated simply by combining individual RFM score numbers.\n",
    "\n",
    "\n",
    "The Code is divided into multiple phases:\n",
    "\n",
    "1. Spark Session Initialization\n",
    "\n",
    "    Initializes a SparkSession with a specified master node and application name \"rfm-segmentation\".\n",
    "    This is the starting point for any Spark application and is used for configuring Spark's settings.\n",
    "\n",
    "2. Data Loading and Initial Processing\n",
    "\n",
    "    Reads an ecommerce CSV file into a Spark DataFrame with a predefined schema and prints the schema to confirm the data structure.\n",
    "    It filters the data to include only purchase events and caches the result for efficient access during further transformations.\n",
    "\n",
    "3. Data Aggregation\n",
    "\n",
    "    Groups the filtered purchase data by \"user_session\" and aggregates it to compute the maximum event time, unique user IDs, count of sessions, and total money spent per session.\n",
    "    This aggregated data is then used to derive insights on user behavior within the ecommerce platform.\n",
    "\n",
    "4. Data Preparation for RFM Analysis\n",
    "\n",
    "    This step prepares the data for RFM segmentation by determining how recently and frequently each user has made purchases and how much they have spent.\n",
    "\n",
    "5. RFM Segmentation Calculation\n",
    "\n",
    "    Performs RFM analysis by grouping data on a user level and calculating Recency, Frequency, and Monetary values.\n",
    "    Determines quantiles for Recency, Frequency, and Monetary to classify users into different segments based on their behavior.\n",
    "\n",
    "6. RFM Score Assignment\n",
    "\n",
    "    Assigns RFM quartiles and scores to each user based on the calculated quantiles. This involves categorizing users into segments such as \"Lost\", \"Hibernating\", etc., based on their RFM scores.\n",
    "    The RFM score is a composite score given to each customer to represent their overall value to the business.\n",
    "\n",
    "7. RFM Labeling and Segmentation Summary\n",
    "\n",
    "    Further refines the RFM segments by assigning descriptive labels based on the RFM score.\n",
    "    Aggregates the final RFM results to calculate average Recency, Frequency, and Monetary values for each segment, along with the size of each segment.\n",
    "\n",
    "8. Visualization\n",
    "\n",
    "    The final part of the code involves creating a visualization of the RFM segments using a squarify plot to represent the size of each segment visually. This helpf for understanding the distribution of users across different RFM segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# für Recency\n",
    "def R(x, q1, q2, q3):\n",
    "    if x <= q1:\n",
    "        return 1\n",
    "    elif x <= q2:\n",
    "        return 2\n",
    "    elif x <= q3:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# für Frequency und Monetary\n",
    "def FM(x, q1, q2, q3):\n",
    "    if x <= q1:\n",
    "        return 4\n",
    "    elif x <= q2:\n",
    "        return 3\n",
    "    elif x <= q3:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def run_rfm_segmentation(mastername, dateiname, schema):\n",
    "    # Memory Usage\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Memory Usage: {memory.percent}%\")\n",
    "\n",
    "    # Disk I/O\n",
    "    disk_io_start = psutil.disk_io_counters()\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(mastername) \\\n",
    "        .appName(\"rfm-segmentation\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Lesen der CSV-Datei mit dem definierten Schema\n",
    "    ecommerce = spark.read.csv(dateiname, schema=schema, header=True)\n",
    "\n",
    "    print(ecommerce.rdd.getNumPartitions())\n",
    "    only_purchases = ecommerce.filter(col(\"event_type\") == 'purchase').cache()\n",
    "\n",
    "    aggregated_data = only_purchases.groupBy(\"user_session\") \\\n",
    "        .agg(\n",
    "            F.max(\"event_time\").alias(\"Date_order\"),\n",
    "            F.collect_set(\"user_id\").alias(\"user_id\"),  # Unique user_ids\n",
    "            F.count(\"user_session\").alias(\"Quantity\"),\n",
    "            F.sum(\"price\").alias(\"money_spent\")\n",
    "        )\n",
    "\n",
    "    # Assuming 'data' is your PySpark DataFrame and 'Date_order' is a string column\n",
    "    study_date = dt.datetime(2019, 12, 1)\n",
    "\n",
    "    # Convert 'Date_order' to date type if it's not already\n",
    "    data = aggregated_data.withColumn(\"Date_order\", F.col(\"Date_order\").cast(DateType()))\n",
    "\n",
    "    # Calculate the difference in days\n",
    "    data = data.withColumn(\"last_purchase\", F.datediff(F.lit(study_date), \"Date_order\"))\n",
    "\n",
    "\n",
    "    RFM_result = data.groupBy(\"user_id\") \\\n",
    "        .agg(\n",
    "            F.min(\"last_purchase\").alias(\"Recency\"),\n",
    "            F.count(\"user_id\").alias(\"Frequency\"),\n",
    "            F.sum(\"money_spent\").alias(\"Monetary\")\n",
    "        )\n",
    "\n",
    "    # Definition der Quantil-Wahrscheinlichkeiten und des relativen Fehlers\n",
    "    quantile_probs = [0.25, 0.5, 0.75]\n",
    "    rel_error = 0.01  # Geringer relativer Fehler für eine genauere Approximation\n",
    "\n",
    "    # Berechnung der Quantile für Recency, Frequency und Monetary\n",
    "    recency_quantiles = RFM_result.stat.approxQuantile(\"Recency\", quantile_probs, rel_error)\n",
    "    frequency_quantiles = RFM_result.stat.approxQuantile(\"Frequency\", quantile_probs, rel_error)\n",
    "    monetary_quantiles = RFM_result.stat.approxQuantile(\"Monetary\", quantile_probs, rel_error)\n",
    "\n",
    "    # Zusammenstellen der Quantilinformation in einem Dictionary\n",
    "    quartiles = {\n",
    "        'Recency': dict(zip(quantile_probs, recency_quantiles)),\n",
    "        'Frequency': dict(zip(quantile_probs, frequency_quantiles)),\n",
    "        'Monetary': dict(zip(quantile_probs, monetary_quantiles))\n",
    "    }\n",
    "\n",
    "    # Extrahieren der Quartilswerte\n",
    "    recency_quartiles = [quartiles['Recency'][0.25], quartiles['Recency'][0.50], quartiles['Recency'][0.75]]\n",
    "    frequency_quartiles = [quartiles['Frequency'][0.25], quartiles['Frequency'][0.50], quartiles['Frequency'][0.75]]\n",
    "    monetary_quartiles = [quartiles['Monetary'][0.25], quartiles['Monetary'][0.50], quartiles['Monetary'][0.75]]\n",
    "\n",
    "    RFM_result = RFM_result.withColumn('R_Quartile', \n",
    "        when(col('Recency') <= recency_quartiles[0], 1)\n",
    "        .when(col('Recency') <= recency_quartiles[1], 2)\n",
    "        .when(col('Recency') <= recency_quartiles[2], 3)\n",
    "        .otherwise(4))\n",
    "\n",
    "    RFM_result = RFM_result.withColumn('F_Quartile', \n",
    "        when(col('Frequency') <= frequency_quartiles[0], 4)\n",
    "        .when(col('Frequency') <= frequency_quartiles[1], 3)\n",
    "        .when(col('Frequency') <= frequency_quartiles[2], 2)\n",
    "        .otherwise(1))\n",
    "\n",
    "    RFM_result = RFM_result.withColumn('M_Quartile', \n",
    "        when(col('Monetary') <= monetary_quartiles[0], 4)\n",
    "        .when(col('Monetary') <= monetary_quartiles[1], 3)\n",
    "        .when(col('Monetary') <= monetary_quartiles[2], 2)\n",
    "        .otherwise(1))\n",
    "\n",
    "    # Erstellen der RFM_segmentation und RFM_score Spalten, indem man die Quartil-Spalten in Strings umwandelt und sie zusammenfügt\n",
    "    RFM_result = RFM_result.withColumn('RFM_segmentation', \n",
    "        col('R_Quartile').cast(StringType()) + \n",
    "        col('F_Quartile').cast(StringType()) + \n",
    "        col('M_Quartile').cast(StringType()))\n",
    "\n",
    "    RFM_result = RFM_result.withColumn('RFM_score', \n",
    "        col('R_Quartile') + \n",
    "        col('F_Quartile') + \n",
    "        col('M_Quartile'))\n",
    "\n",
    "\n",
    "    RFM_result = RFM_result.withColumn('RFM_label', \n",
    "        when(col('RFM_score') >= 10, 'Lost')\n",
    "        .when(col('RFM_score') >= 9, 'Hibernating')\n",
    "        .when(col('RFM_score') >= 8, 'Can’t Lose Them')\n",
    "        .when(col('RFM_score') >= 7, 'About To Sleep')\n",
    "        .when(col('RFM_score') >= 6, 'Promising')\n",
    "        .when(col('RFM_score') >= 5, 'Potential Loyalist')\n",
    "        .when(col('RFM_score') >= 4, 'Loyal Customers')\n",
    "        .otherwise('Champions'))\n",
    "\n",
    "    # Gruppieren nach 'RFM_label' und Berechnen der Durchschnittswerte sowie der Gruppengröße\n",
    "    RFM_desc = RFM_result.groupBy('RFM_label').agg(\n",
    "        F.mean('Recency').alias('Average_Recency'),\n",
    "        F.mean('Frequency').alias('Average_Frequency'),\n",
    "        F.mean('Monetary').alias('Average_Monetary'),\n",
    "        F.count('RFM_label').alias('Segment_Size')\n",
    "    )\n",
    "\n",
    "    # Runden der Durchschnittswerte auf eine Dezimalstelle\n",
    "    RFM_desc = RFM_desc.select(\n",
    "        'RFM_label',\n",
    "        F.round('Average_Recency', 1).alias('Average_Recency'),\n",
    "        F.round('Average_Frequency', 1).alias('Average_Frequency'),\n",
    "        F.round('Average_Monetary', 1).alias('Average_Monetary'),\n",
    "        'Segment_Size'\n",
    "    )\n",
    "\n",
    "    # Anzeigen des aggregierten DataFrames\n",
    "    RFM_desc.show()\n",
    "\n",
    "    # Extract the necessary columns from the Spark DataFrame\n",
    "    sizes = RFM_desc.select(\"Segment_Size\").rdd.flatMap(lambda x: x).collect()\n",
    "    labels = RFM_desc.select(\"RFM_label\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    # Create your plot and resize it\n",
    "    #fig = plt.gcf()\n",
    "    #ax = fig.add_subplot()\n",
    "    #fig.set_size_inches(16, 9)\n",
    "\n",
    "    # Create squarify plot\n",
    "    #squarify.plot(sizes=sizes, label=labels, alpha=.6)\n",
    "    #plt.title(\"RFM Segments\", fontsize=18, fontweight=\"bold\")\n",
    "    #plt.axis('off')\n",
    "    \n",
    "    disk_io_end = psutil.disk_io_counters()\n",
    "\n",
    "    read_bytes = disk_io_end.read_bytes - disk_io_start.read_bytes\n",
    "    write_bytes = disk_io_end.write_bytes - disk_io_start.write_bytes\n",
    "\n",
    "    print(f\"Read: {read_bytes / 1024 / 1024:.2f} MB, Write: {write_bytes / 1024 / 1024:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scaling (Cluster)\n",
    "\n",
    "In this section we want to provide an overview for scaling our spark application based on a local standalone cluster setup\n",
    "\n",
    "Therefore we compare the total time of the application and the important metrics of the longest job of the aplication based on different cluster configurations.\n",
    "\n",
    "To accurately setup the test invorement for the cluster, use the terminal to create the master node and the worker node(s)\n",
    "The following code has to be excecuted in the bin folder of your spark installation.\n",
    "\n",
    "master node:\n",
    "spark-class org.apache.spark.deploy.master.Master\n",
    "\n",
    "worker node:\n",
    "spark-class org.apache.spark.deploy.worker.Worker spark://<masternode-ip>:<masternode-port>\n",
    "\n",
    "The base configuration for worker nodes is 15.0 GiB (1024.0 MiB Used) of RAM and 10 Cores .\n",
    "The worker can be configured by using --cores x for the number of cores and --memory x for the associated RAM.\n",
    "\n",
    "e.g.\n",
    "\n",
    "./spark-class org.apache.spark.deploy.worker.Worker \\\n",
    "    --cores 5 \\\n",
    "    spark://100.119.9.7:7077\n",
    "\n",
    "./spark-class org.apache.spark.deploy.worker.Worker \\\n",
    "    --memory 512m \\\n",
    "    spark://100.119.9.7:7077\n",
    "\n",
    "After creating the cluster, change the \"spark://<masternode-ip>:<masternode-port>\" of the cluster the test according to your cluster.\n",
    "\n",
    "The setup for the test cases is as followed:\n",
    "\n",
    "master1Worker:\n",
    "- Base configuration\n",
    "- 1 worker\n",
    "\n",
    "master2Worker:\n",
    "- Base configuration\n",
    "- 2 workers\n",
    "\n",
    "master3Worker:\n",
    "- Base\n",
    "- 3 workers\n",
    "\n",
    "master6Worker:\n",
    "- Base\n",
    "- 6 workers\n",
    "\n",
    "master2Core:\n",
    "- --cores 2\n",
    "- 3 workers\n",
    "\n",
    "master5Core:\n",
    "- --cores 5\n",
    "- 3 workers\n",
    "\n",
    "master256MB:\n",
    "- --memory 256m\n",
    "- 3 workers\n",
    "\n",
    "master512MB:\n",
    "- --memory 512m\n",
    "- 3 workers\n",
    "\n",
    "master768MB:\n",
    "- --memory 768m\n",
    "- 3 workers\n",
    "\n",
    "Change the path to the data according to your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with different sizes\n",
    "smallData = '../../../Testdaten/only_purchases_1day.csv'\n",
    "mediumData = '../../../2019-Oct.csv'\n",
    "bigData = '../../../*.csv'\n",
    "\n",
    "# the clusters for the tests\n",
    "master1Worker = \"spark://100.119.9.7:7077\"\n",
    "master2Worker = \"spark://100.119.9.7:7077\"\n",
    "master3Worker = \"spark://100.119.9.7:7077\"\n",
    "master6Worker = \"spark://100.119.9.7:7077\"\n",
    "master2Core = \"spark://100.119.9.7:7077\"\n",
    "master5Core = \"spark://100.119.9.7:7077\"\n",
    "master256MB = \"spark://100.119.9.7:7077\"\n",
    "master512MB = \"spark://100.119.9.7:7077\"\n",
    "master768MB = \"spark://100.119.9.7:7077\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scalability (Cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Small Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage: 59.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/02 16:01:39 WARN Utils: Your hostname, Nikolais-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 100.119.9.7 instead (on interface en0)\n",
      "24/02/02 16:01:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/02 16:01:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-----------------+----------------+------------+\n",
      "|         RFM_label|Average_Recency|Average_Frequency|Average_Monetary|Segment_Size|\n",
      "+------------------+---------------+-----------------+----------------+------------+\n",
      "|         Champions|           61.0|              2.6|          1808.0|         869|\n",
      "|         Promising|           61.0|              1.1|           998.6|        2860|\n",
      "|Potential Loyalist|           61.0|              2.2|           163.3|         165|\n",
      "|       Hibernating|           61.0|              1.0|            48.9|        3436|\n",
      "|   Loyal Customers|           61.0|              2.2|           358.3|         391|\n",
      "|   Can’t Lose Them|           61.0|              1.0|           155.8|        3274|\n",
      "|    About To Sleep|           61.0|              1.0|           327.0|        3069|\n",
      "+------------------+---------------+-----------------+----------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read: 20.52 MB, Write: 44.16 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "run_rfm_segmentation(master3Worker, smallData, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Medium Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rfm_segmentation(master3Worker, mediumData, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rfm_segmentation(master3Worker, bigData, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion Data Scalability (Cluster)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Scalabilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rfm_segmentation(master1Worker, bigData, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rfm_segmentation(master2Worker, bigData, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rfm_segmentation(master6Worker, bigData, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rfm_segmentation(master2Core, bigData, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rfm_segmentation(master5Core, bigData, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rfm_segmentation(master256MB, bigData, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rfm_segmentation(master512MB, bigData, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_rfm_segmentation(master768MB, bigData, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion Cluster Scalability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
