{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fault Tolerance\n",
    "\n",
    "This section aims to explore the following questions:\n",
    "- How does the system behave under Node/CPU/Memory/Hardware/... errors and failures?\n",
    "- What happens during network interruptions and partitioning?\n",
    "- How do error handling mechanisms affect efficiency/scale/latency/throughput/... etc.?\n",
    "\n",
    "Therefore, we will observe Spark's fault tolerance mechanism by forcibly stopping a Java process.\n",
    " \n",
    "We set up a Spark Cluster in Standalone mode, consisting of one master and three worker nodes, and start a Spark Application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sql\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum,avg,max,count\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/31 12:25:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"spark://192.168.0.5:7077\").appName(\"fault-tolerance4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "root = '../../../Data/eCommerce-behavior-data/2019-Oct.csv'\n",
    "# root = '../../data/only_purchases_1day.csv'\n",
    "ecommerce = spark.read\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce.createOrReplaceTempView('ecommerce_2019_oct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executors\n",
    "![Image](https://i.imgur.com/TEtFIbe.png)\n",
    "You can observe through the Spark UI that there are 4 Executors, namely Master, Worker1, Worker2, and Worker3. \n",
    "\n",
    "Each Executor has two Java processes, resulting in a total of 8 Java processes in execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of executors: 4\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "# get the number of executors\n",
    "num_executors = sc._jsc.sc().getExecutorMemoryStatus().size()\n",
    "print(\"Number of executors:\", num_executors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill Executor\n",
    "During the execution of a job, if an Executor encounters an exception, Spark takes the following actions:\n",
    "- When an exception occurs in the Executor, the external wrapper class ExecutorRunner *sends the exception message to the Worker*.\n",
    "- Subsequently, the Worker *sends a message to the Master*.\n",
    "- Upon receiving the Executor status change message, if the Master detects an abnormal exit of the Executor, it invokes the Master.schedule method to *attempt to obtain an available Worker node* and restart the Executor.\n",
    "\n",
    "### Narrow Dependency\n",
    "In the case of *Narrow Dependency*, as each parent RDD partition depends on a specific child RDD partition, *the data from this child RDD partition can be directly used during recomputation*, avoiding Redundant Computation.\n",
    "\n",
    "We conduct an experiment using the `filter` operation:\n",
    "\n",
    "However, from the Spark UI, we can observe the following behavior. It triggers two jobs: \n",
    "1. The first job executes the `filter`\n",
    "2. The second job executes the `count`. \n",
    "\n",
    "Since the `filter` operation is executed first and the result is stored in the cache, the second job skips the reading of the CSV step and directly reads the data from the cache for counting. You can see in the image that part of Stage 5 is skipped. Typically, it means that data has been fetched from the cache, and there was no need to re-execute the given stage.\n",
    "\n",
    "![Image](https://i.imgur.com/dyulBi4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not skilling executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=====================================================>   (40 + 3) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many purchase session in one month: 742849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "only_purchases = ecommerce.filter(col(\"event_type\") == 'purchase')\n",
    "print(\"How many purchase session in one month:\", only_purchases.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **After examining the execution status under the condition where no errors occurred, we intentionally force stop one Executor's Java Process by repeatedly executing the same steps.**\n",
    "> \n",
    "> **We observe the behavior of Spark in the case of narrow dependency execution (Filter):**\n",
    "\n",
    "![Image](https://i.imgur.com/o8xf2hx.png)\n",
    "\n",
    "In the following code snippet, I forcefully stop one of the Java Processes of an Executor using the Activity Monitor.\n",
    "As mentioned in the previous step, one executor has two Java Processes. By stopping one of them, the entire executor's work comes to a halt.\n",
    "This action will result in the following:\n",
    "1. Removal of a specific Executor.\n",
    "2. Redistribution of the pending tasks of the stopped Executor to other Executors.\n",
    "3. The final result remains the same as the original, with a quantity of 742.849. Because the tasks are redistributed to other Executors, the result is the same as the original.\n",
    "\n",
    "However, the worker node will be marked as dead due to the forced termination of the Java process.\n",
    "It is necessary to restart the worker node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 00:58:00 ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.0.5: Command exited with code 143\n",
      "24/01/31 00:58:00 WARN TaskSetManager: Lost task 31.0 in stage 5.0 (TID 119) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 00:58:00 WARN TaskSetManager: Lost task 39.0 in stage 5.0 (TID 127) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 00:58:00 WARN TaskSetManager: Lost task 41.0 in stage 5.0 (TID 129) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 00:58:00 WARN TaskSetManager: Lost task 38.0 in stage 5.0 (TID 126) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "[Stage 5:>                                                        (0 + 20) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many purchase session in one month: 742849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "only_purchases = ecommerce.filter(col(\"event_type\") == 'purchase')\n",
    "print(\"How many purchase session in one month:\", only_purchases.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide Dependency\n",
    "- In the case of **wide dependency**, when an entire subRDD partition is lost, Spark must **recompute all the parent RDDs associated with that subRDD partition**, as **multiple parent RDD partitions may depend on this subRDD partition**.\n",
    "- In scenarios with a long compute chain and wide dependency, it is recommended to perform a checkpoint or caching to store intermediate results, reducing execution overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not skilling executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================================>           (34 + 9) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "|        user_session|         Date_order|    user_id|Quantity|       money_spent|\n",
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "|000081ea-9376-4eb...|2019-10-24 11:08:58|[513622224]|       1|            131.51|\n",
      "|000723e7-1ff9-484...|2019-10-05 15:21:09|[543470009]|       1|             49.36|\n",
      "|000941cc-a55d-4a5...|2019-10-24 22:20:26|[563830578]|       1|              40.9|\n",
      "|00095607-9518-42c...|2019-10-05 19:05:28|[531516671]|       1|            386.08|\n",
      "|000a2754-1167-47c...|2019-10-28 12:56:13|[554129220]|       1|             39.68|\n",
      "|0010e63b-0333-4f6...|2019-10-16 14:57:29|[525771398]|       1|             31.64|\n",
      "|00149062-a045-4a1...|2019-10-26 22:11:53|[558054947]|       2|113.50999999999999|\n",
      "|00167766-6565-4b6...|2019-10-30 09:50:10|[565693206]|       1|            385.83|\n",
      "|0016bf0d-cdc0-4d6...|2019-10-17 13:17:20|[550091025]|       1|            242.07|\n",
      "|001a1e22-7e00-4de...|2019-10-14 11:07:53|[559044395]|       1|            434.73|\n",
      "|001baa62-ade8-40e...|2019-10-30 11:53:30|[563170001]|       1|           1207.32|\n",
      "|0023b834-427c-4da...|2019-10-23 03:53:03|[512996254]|       1|              24.7|\n",
      "|0026da65-d60b-4d7...|2019-10-31 18:15:04|[514674061]|       1|           1543.39|\n",
      "|0028a99a-47d4-4a8...|2019-10-05 07:32:02|[534917019]|       1|            308.37|\n",
      "|002fc82e-064a-4cd...|2019-10-31 12:15:03|[530221552]|       1|            1667.7|\n",
      "|003144e8-b946-4b3...|2019-10-14 12:39:32|[547765693]|       1|            130.71|\n",
      "|00323239-8ffc-45e...|2019-10-07 09:36:18|[517111916]|       1|             84.66|\n",
      "|00326296-8877-47f...|2019-10-11 04:34:33|[538672167]|       1|            434.99|\n",
      "|0032e941-8533-46a...|2019-10-16 09:43:52|[513593613]|       1|             58.95|\n",
      "|00331d66-8566-4dc...|2019-10-08 07:18:53|[543356214]|       1|            130.76|\n",
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "aggregated_data = only_purchases.groupBy(\"user_session\") \\\n",
    "    .agg(\n",
    "        F.max(\"event_time\").alias(\"Date_order\"),\n",
    "        F.collect_set(\"user_id\").alias(\"user_id\"),  # Unique user_ids\n",
    "        F.count(\"user_session\").alias(\"Quantity\"),\n",
    "        F.sum(\"price\").alias(\"money_spent\")\n",
    "    )\n",
    "aggregated_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skilling Executor\n",
    "\n",
    "\n",
    "![Image](https://i.imgur.com/7Tec3Hp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 00:44:52 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.0.5: Command exited with code 143\n",
      "24/01/31 00:44:52 WARN TaskSetManager: Lost task 30.0 in stage 10.0 (TID 164) (192.168.0.5 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 00:44:52 WARN TaskSetManager: Lost task 42.0 in stage 10.0 (TID 176) (192.168.0.5 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 00:44:52 WARN TaskSetManager: Lost task 33.0 in stage 10.0 (TID 167) (192.168.0.5 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 00:44:52 WARN TaskSetManager: Lost task 35.0 in stage 10.0 (TID 169) (192.168.0.5 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 00:44:52 WARN TaskSetManager: Lost task 38.0 in stage 10.0 (TID 172) (192.168.0.5 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 00:44:52 WARN TaskSetManager: Lost task 31.0 in stage 10.0 (TID 165) (192.168.0.5 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 00:44:52 WARN TaskSetManager: Lost task 34.0 in stage 10.0 (TID 168) (192.168.0.5 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "[Stage 10:============================>                            (5 + 5) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "|        user_session|         Date_order|    user_id|Quantity|       money_spent|\n",
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "|000081ea-9376-4eb...|2019-10-24 11:08:58|[513622224]|       1|            131.51|\n",
      "|000174ac-0ea3-402...|2019-10-18 12:46:20|[548449052]|       2|            499.72|\n",
      "|0004400f-dc39-410...|2019-10-16 07:24:33|[550005829]|       1|            143.63|\n",
      "|0004c309-ff34-44b...|2019-10-13 13:59:14|[547022478]|       2|             281.2|\n",
      "|000723e7-1ff9-484...|2019-10-05 15:21:09|[543470009]|       1|             49.36|\n",
      "|000941cc-a55d-4a5...|2019-10-24 22:20:26|[563830578]|       1|              40.9|\n",
      "|00095607-9518-42c...|2019-10-05 19:05:28|[531516671]|       1|            386.08|\n",
      "|000a2754-1167-47c...|2019-10-28 12:56:13|[554129220]|       1|             39.68|\n",
      "|000a9525-b9a4-4cf...|2019-10-07 18:54:17|[557779190]|       1|            102.71|\n",
      "|000cbc68-4323-4ba...|2019-10-13 08:35:34|[533715732]|       1|             92.09|\n",
      "|000e2bba-bbf2-432...|2019-10-10 10:51:47|[541825176]|       1|             88.81|\n",
      "|0010e63b-0333-4f6...|2019-10-16 14:57:29|[525771398]|       1|             31.64|\n",
      "|00149062-a045-4a1...|2019-10-26 22:11:53|[558054947]|       2|113.50999999999999|\n",
      "|00167766-6565-4b6...|2019-10-30 09:50:10|[565693206]|       1|            385.83|\n",
      "|0016bf0d-cdc0-4d6...|2019-10-17 13:17:20|[550091025]|       1|            242.07|\n",
      "|001868b0-06b7-40a...|2019-10-20 18:13:30|[546693089]|       2|             336.6|\n",
      "|001a1e22-7e00-4de...|2019-10-14 11:07:53|[559044395]|       1|            434.73|\n",
      "|001baa62-ade8-40e...|2019-10-30 11:53:30|[563170001]|       1|           1207.32|\n",
      "|001c4f5a-ab44-45f...|2019-10-12 11:51:23|[521009984]|       1|             36.01|\n",
      "|001de77d-9b9a-4c5...|2019-10-28 17:13:36|[564395096]|       2|            261.98|\n",
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "aggregated_data = only_purchases.groupBy(\"user_session\") \\\n",
    "    .agg(\n",
    "        F.max(\"event_time\").alias(\"Date_order\"),\n",
    "        F.collect_set(\"user_id\").alias(\"user_id\"),  # Unique user_ids\n",
    "        F.count(\"user_session\").alias(\"Quantity\"),\n",
    "        F.sum(\"price\").alias(\"money_spent\")\n",
    "    )\n",
    "aggregated_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Before Killing Executor\n",
    "Using cache to observe the difference when executor is killed.\n",
    "After caching the data, killing an executor, and observing Spark's behavior:\n",
    "\n",
    "**Error Message:**\n",
    "```python\n",
    "Lost executor 1 on 192.168.0.5: Command exited with code 143\n",
    "```\n",
    "\n",
    "- This indicates that Spark lost an executor, and the lost executor is Executor 1 on IP address 192.168.0.5.\n",
    "- The error code 143 indicates that the process was terminated, typically due to a system-sent interrupt signal.\n",
    "\n",
    "**Warning Message:**\n",
    "```python\n",
    "Lost task 30.0 in stage 6.0 (TID 161) (192.168.0.5 executor 1): \n",
    "ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
    "```\n",
    "\n",
    "- This is a warning message about the failure of task 30.0 in Stage 6.0.\n",
    "- It indicates that the task was running on Executor 1, and due to the exit of that executor, the task also failed.\n",
    "- The reason is that the command execution returned exit code 143.\n",
    "\n",
    "```python\n",
    "No more replicas available for rdd_21_30!\n",
    "```\n",
    "- This is a warning related to the RDD (rdd_21_30), indicating that no more replicas are available. This might be due to the loss of an executor, resulting in the loss of some partitions of the RDD.\n",
    "- You may notice that when performing executor kill with and without caching, an additional error message appears:`No more replicas available for rdd...`\n",
    "  - When we use cache or persist to cache data in Spark, Spark attempts to create replicas of the data on multiple nodes in the cluster to enhance data redundancy and reliability. \n",
    "  - This way, even if an Executor stops, there are still copies available on other nodes.\n",
    "  - However, because the default replication factor is 1. Therefore, when an executor is killed, this error occurs as there are no other replicas available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[event_time: timestamp, event_type: string, product_id: int, category_id: bigint, category_code: string, brand: string, price: double, user_id: int, user_session: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_purchases.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 01:16:15 ERROR TaskSchedulerImpl: Lost executor 1 on 192.168.0.5: Command exited with code 143\n",
      "24/01/31 01:16:15 WARN TaskSetManager: Lost task 18.0 in stage 6.0 (TID 151) (192.168.0.5 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 01:16:15 WARN TaskSetManager: Lost task 12.0 in stage 6.0 (TID 145) (192.168.0.5 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 01:16:15 WARN TaskSetManager: Lost task 21.0 in stage 6.0 (TID 154) (192.168.0.5 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 01:16:15 WARN TaskSetManager: Lost task 6.0 in stage 6.0 (TID 139) (192.168.0.5 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 01:16:15 WARN TaskSetManager: Lost task 35.0 in stage 6.0 (TID 162) (192.168.0.5 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 01:16:15 WARN TaskSetManager: Lost task 30.0 in stage 6.0 (TID 161) (192.168.0.5 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_30 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_24 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_6 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_3 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_12 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_18 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_9 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_35 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_15 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_21 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_27 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_0 !\n",
      "24/01/31 01:16:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_39 !\n",
      "[Stage 6:======================================>                  (16 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "|        user_session|         Date_order|    user_id|Quantity|       money_spent|\n",
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "|000081ea-9376-4eb...|2019-10-24 11:08:58|[513622224]|       1|            131.51|\n",
      "|000174ac-0ea3-402...|2019-10-18 12:46:20|[548449052]|       2|            499.72|\n",
      "|0004400f-dc39-410...|2019-10-16 07:24:33|[550005829]|       1|            143.63|\n",
      "|0004c309-ff34-44b...|2019-10-13 13:59:14|[547022478]|       2|             281.2|\n",
      "|000723e7-1ff9-484...|2019-10-05 15:21:09|[543470009]|       1|             49.36|\n",
      "|000941cc-a55d-4a5...|2019-10-24 22:20:26|[563830578]|       1|              40.9|\n",
      "|00095607-9518-42c...|2019-10-05 19:05:28|[531516671]|       1|            386.08|\n",
      "|000a2754-1167-47c...|2019-10-28 12:56:13|[554129220]|       1|             39.68|\n",
      "|000a9525-b9a4-4cf...|2019-10-07 18:54:17|[557779190]|       1|            102.71|\n",
      "|000cbc68-4323-4ba...|2019-10-13 08:35:34|[533715732]|       1|             92.09|\n",
      "|000e2bba-bbf2-432...|2019-10-10 10:51:47|[541825176]|       1|             88.81|\n",
      "|0010e63b-0333-4f6...|2019-10-16 14:57:29|[525771398]|       1|             31.64|\n",
      "|00149062-a045-4a1...|2019-10-26 22:11:53|[558054947]|       2|113.50999999999999|\n",
      "|00167766-6565-4b6...|2019-10-30 09:50:10|[565693206]|       1|            385.83|\n",
      "|0016bf0d-cdc0-4d6...|2019-10-17 13:17:20|[550091025]|       1|            242.07|\n",
      "|001868b0-06b7-40a...|2019-10-20 18:13:30|[546693089]|       2|             336.6|\n",
      "|001a1e22-7e00-4de...|2019-10-14 11:07:53|[559044395]|       1|            434.73|\n",
      "|001baa62-ade8-40e...|2019-10-30 11:53:30|[563170001]|       1|           1207.32|\n",
      "|001c4f5a-ab44-45f...|2019-10-12 11:51:23|[521009984]|       1|             36.01|\n",
      "|001de77d-9b9a-4c5...|2019-10-28 17:13:36|[564395096]|       2|            261.98|\n",
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 01:18:13 ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.0.5: Worker shutting down\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_30 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_24 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_42 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_26 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_6 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_5 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_29 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_37 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_12 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_18 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_9 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_17 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_8 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_14 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_20 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_33 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_23 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_11 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_0 !\n",
      "24/01/31 01:18:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_2 !\n",
      "24/01/31 01:18:19 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.0.5: Command exited with code 130\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_38 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_13 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_10 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_35 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_21 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_4 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_36 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_7 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_39 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_25 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_22 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_3 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_32 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_34 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_41 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_15 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_19 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_16 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_27 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_28 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_1 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_40 !\n",
      "24/01/31 01:18:19 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_21_31 !\n",
      "24/01/31 01:18:21 WARN StandaloneAppClient$ClientEndpoint: Connection to 192.168.0.5:7077 failed; waiting for master to reconnect...\n",
      "24/01/31 01:18:21 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n",
      "24/01/31 01:18:21 WARN StandaloneAppClient$ClientEndpoint: Connection to 192.168.0.5:7077 failed; waiting for master to reconnect...\n"
     ]
    }
   ],
   "source": [
    "aggregated_data = only_purchases.groupBy(\"user_session\") \\\n",
    "    .agg(\n",
    "        F.max(\"event_time\").alias(\"Date_order\"),\n",
    "        F.collect_set(\"user_id\").alias(\"user_id\"),  # Unique user_ids\n",
    "        F.count(\"user_session\").alias(\"Quantity\"),\n",
    "        F.sum(\"price\").alias(\"money_spent\")\n",
    "    )\n",
    "aggregated_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist Before Killing Executor\n",
    "\n",
    "Here, we compare the `persist()` and `cache()` methods. Using the `persist()` method,\n",
    "\n",
    "### `Persist()` vs `Cache()`\n",
    "\n",
    "**Persist()**\n",
    "\n",
    "1. `persist()` is a more general method and allows specifying more options.\n",
    "2. The `persist()` method can set the storage level by specifying the `StorageLevel` parameter. For example, you can choose from options like `MEMORY_ONLY`, `MEMORY_ONLY_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`, `MEMORY_AND_DISK_2`, etc.\n",
    "   1. `MEMORY_ONLY_SER`: Serializes the storage in memory, reducing memory usage. If the memory storage is not sufficient to hold all RDD blocks, Spark won't cache the data and won't throw an error. This can result in expensive recomputation if the RDD is needed again.\n",
    "   2. `MEMORY_AND_DISK_SER`: Serializes the storage in memory and on disk, reducing memory usage. In industry setups, `persist(Storage_Level.MEMORY_AND_DISK)` is commonly used because it combines the benefits of caching in memory and spilling to disk when memory is limited.\n",
    "   3. `MEMORY_ONLY_2`: Sets replication factor as 2, creating replicas of each partition on two nodes in the cluster.\n",
    "   4. `MEMORY_AND_DISK_2`: Sets replication factor as 2, meaning it stores two copies of each block in memory and on disk.\n",
    "\n",
    "**Cache()**\n",
    "\n",
    "1. `cache()` is a specific case of `persist()`, and it is equivalent to `.persist(StorageLevel.MEMORY_ONLY)`.\n",
    "2. The `cache()` method does not provide detailed parameter options like `persist()`; it simply caches the data in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[event_time: timestamp, event_type: string, product_id: int, category_id: bigint, category_code: string, brand: string, price: double, user_id: int, user_session: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "only_purchases.persist(StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that by creating replicas on two nodes, the warning message `WARN BlockManagerMasterEndpoint: No more replicas available for rdd_...` does not occur.\n",
    "\n",
    "So, when a node fails, the driver spawns another executor on a different node and provides it with the data partition on which it was supposed to work, along with the associated Directed Acyclic Graph (DAG) in a closure. With this information, it can recompute the data and materialize it.\n",
    "\n",
    "In the meantime, the cached data in the Resilient Distributed Dataset (RDD) won't have all the data in memory. The data of the lost nodes needs to be fetched from the disk, which will take a little more time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 12:18:26 ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.0.5: Command exited with code 143\n",
      "24/01/31 12:18:26 WARN TaskSetManager: Lost task 26.0 in stage 6.0 (TID 156) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 12:18:26 WARN TaskSetManager: Lost task 16.0 in stage 6.0 (TID 147) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 12:18:26 WARN TaskSetManager: Lost task 7.0 in stage 6.0 (TID 138) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 12:18:26 WARN TaskSetManager: Lost task 28.0 in stage 6.0 (TID 159) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 12:18:26 WARN TaskSetManager: Lost task 1.0 in stage 6.0 (TID 132) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 12:18:26 WARN TaskSetManager: Lost task 20.0 in stage 6.0 (TID 150) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 12:18:26 WARN TaskSetManager: Lost task 11.0 in stage 6.0 (TID 141) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 12:18:26 WARN TaskSetManager: Lost task 13.0 in stage 6.0 (TID 144) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 12:18:26 WARN TaskSetManager: Lost task 23.0 in stage 6.0 (TID 153) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "24/01/31 12:18:26 WARN TaskSetManager: Lost task 4.0 in stage 6.0 (TID 135) (192.168.0.5 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Command exited with code 143\n",
      "[Stage 6:==============================================>          (35 + 8) / 43]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "|        user_session|         Date_order|    user_id|Quantity|       money_spent|\n",
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "|000081ea-9376-4eb...|2019-10-24 11:08:58|[513622224]|       1|            131.51|\n",
      "|000174ac-0ea3-402...|2019-10-18 12:46:20|[548449052]|       2|            499.72|\n",
      "|0004400f-dc39-410...|2019-10-16 07:24:33|[550005829]|       1|            143.63|\n",
      "|0004c309-ff34-44b...|2019-10-13 13:59:14|[547022478]|       2|             281.2|\n",
      "|000723e7-1ff9-484...|2019-10-05 15:21:09|[543470009]|       1|             49.36|\n",
      "|000941cc-a55d-4a5...|2019-10-24 22:20:26|[563830578]|       1|              40.9|\n",
      "|00095607-9518-42c...|2019-10-05 19:05:28|[531516671]|       1|            386.08|\n",
      "|000a2754-1167-47c...|2019-10-28 12:56:13|[554129220]|       1|             39.68|\n",
      "|000a9525-b9a4-4cf...|2019-10-07 18:54:17|[557779190]|       1|            102.71|\n",
      "|000cbc68-4323-4ba...|2019-10-13 08:35:34|[533715732]|       1|             92.09|\n",
      "|000e2bba-bbf2-432...|2019-10-10 10:51:47|[541825176]|       1|             88.81|\n",
      "|0010e63b-0333-4f6...|2019-10-16 14:57:29|[525771398]|       1|             31.64|\n",
      "|00149062-a045-4a1...|2019-10-26 22:11:53|[558054947]|       2|113.50999999999999|\n",
      "|00167766-6565-4b6...|2019-10-30 09:50:10|[565693206]|       1|            385.83|\n",
      "|0016bf0d-cdc0-4d6...|2019-10-17 13:17:20|[550091025]|       1|            242.07|\n",
      "|001868b0-06b7-40a...|2019-10-20 18:13:30|[546693089]|       2|             336.6|\n",
      "|001a1e22-7e00-4de...|2019-10-14 11:07:53|[559044395]|       1|            434.73|\n",
      "|001baa62-ade8-40e...|2019-10-30 11:53:30|[563170001]|       1|           1207.32|\n",
      "|001c4f5a-ab44-45f...|2019-10-12 11:51:23|[521009984]|       1|             36.01|\n",
      "|001de77d-9b9a-4c5...|2019-10-28 17:13:36|[564395096]|       2|            261.98|\n",
      "+--------------------+-------------------+-----------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "aggregated_data = only_purchases.groupBy(\"user_session\") \\\n",
    "    .agg(\n",
    "        F.max(\"event_time\").alias(\"Date_order\"),\n",
    "        F.collect_set(\"user_id\").alias(\"user_id\"),  # Unique user_ids\n",
    "        F.count(\"user_session\").alias(\"Quantity\"),\n",
    "        F.sum(\"price\").alias(\"money_spent\")\n",
    "    )\n",
    "aggregated_data.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
