{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import findspark \n",
    "findspark.init()\n",
    "\n",
    "# for sql\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum,avg,max,count\n",
    "\n",
    "# for time \n",
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/21 23:51:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# 可以改成 *.csv \n",
    "root = '../../*.csv'\n",
    "spark = SparkSession.builder.appName('eCommerce').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "ecommerce = spark.read\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce.createOrReplaceTempView('ecommerce_2019_oct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ecommerce.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+------+------+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code| brand| price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+--------------------+------+------+---------+--------------------+\n",
      "|2019-11-01 01:00:00|      view|   1003461|2053013555631882655|electronics.smart...|xiaomi|489.07|520088904|4d3b30da-a5e4-49d...|\n",
      "|2019-11-01 01:00:00|      view|   5000088|2053013566100866035|appliances.sewing...|janome|293.65|530496790|8e5f4f83-366c-4f7...|\n",
      "|2019-11-01 01:00:01|      view|  17302664|2053013553853497655|                NULL| creed| 28.31|561587266|755422e7-9040-477...|\n",
      "|2019-11-01 01:00:01|      view|   3601530|2053013563810775923|appliances.kitche...|    lg|712.87|518085591|3bfb58cd-7892-48c...|\n",
      "|2019-11-01 01:00:01|      view|   1004775|2053013555631882655|electronics.smart...|xiaomi|183.27|558856683|313628f1-68b8-460...|\n",
      "+-------------------+----------+----------+-------------------+--------------------+------+------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ecommerce.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with smaller (day) to larger size (months) and show when the time takes to long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Smaller days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[avg(price#23)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=49]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(price#23)])\n",
      "         +- Project [price#23]\n",
      "            +- Filter (isnotnull(event_time#17) AND (event_time#17 < 2019-10-02 00:00:00))\n",
      "               +- FileScan csv [event_time#17,price#23] Batched: false, DataFilters: [isnotnull(event_time#17), (event_time#17 < 2019-10-02 00:00:00)], Format: CSV, Location: InMemoryFileIndex(2 paths)[file:/Users/clara/hka_code/2019-Nov.csv, file:/Users/clara/hka_code/20..., PartitionFilters: [], PushedFilters: [IsNotNull(event_time), LessThan(event_time,2019-10-02 00:00:00.0)], ReadSchema: struct<event_time:timestamp,price:double>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=====================================================> (106 + 4) / 110]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average price: 297.6879048473118\n",
      "Time taken: 78.49041295051575 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 使用 SQL 查询< '2019-10-02'\n",
    "query = \"\"\"\n",
    "SELECT AVG(price) FROM ecommerce_2019_oct\n",
    "WHERE event_time < '2019-10-02'\n",
    "\"\"\"\n",
    "result = spark.sql(query)\n",
    "spark.sql(query).explain()\n",
    "\n",
    "start = time.time()\n",
    "avg_price = result.collect()[0][0]\n",
    "print(f\"Average price: {avg_price}\")\n",
    "print(f'Time taken: {time.time() - start} sec')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- larger size (whole month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[avg(price#23)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=108]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(price#23)])\n",
      "         +- FileScan csv [price#23] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(2 paths)[file:/Users/clara/hka_code/2019-Nov.csv, file:/Users/clara/hka_code/20..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<price:double>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=====================================================> (106 + 4) / 110]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average price: 291.63480233260026\n",
      "Time taken: 39.81396293640137 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 查询整個\n",
    "query_all = \"SELECT AVG(price) FROM ecommerce_2019_oct\"\n",
    "result_all = spark.sql(query_all)\n",
    "spark.sql(query_all).explain()\n",
    "start = time.time() \n",
    "avg_all_price = result_all.collect()[0][0]\n",
    "print(f\"Average price: {avg_all_price}\")\n",
    "print(f'Time taken: {time.time() - start} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================================>  (105 + 5) / 110]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        avg(price)|\n",
      "+------------------+\n",
      "|291.63480233260026|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_all.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make it faster with using ... \n",
    "1. Increase the number of parallel partitions \n",
    "2. Using cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame to RDD to check the number of partitions\n",
    "print(\"Partition count:\", ecommerce.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce = ecommerce.repartition(3000)\n",
    "start_date = \"2019-10-01\"\n",
    "end_date = \"2019-10-02\"\n",
    "\n",
    "# For one day\n",
    "specified_period = ecommerce.filter((col(\"event_time\") >= start_date) & (col(\"event_time\") < end_date))\n",
    "print(f'Number of rows: {specified_period.count()}')\n",
    "start = time.time() \n",
    "average_price_specified_period = ecommerce.agg(avg(\"price\")).collect()\n",
    "print(f'average_price_specified_period: {average_price_specified_period}')\n",
    "print(f'Average price for specified period ({start_date} to {end_date}): {average_price_specified_period[0][0]}')\n",
    "print(f'Time taken: {time.time() - start} sec')\n",
    "\n",
    "\n",
    "# For whole period\n",
    "start = time.time() \n",
    "print(f'Number of rows: {ecommerce.count()}')\n",
    "average_proce_all = ecommerce.agg(avg(\"price\")).collect()\n",
    "print(f'average_proce_all: {average_proce_all}')\n",
    "print(f'Average price for whole period: {average_proce_all[0][0]}')\n",
    "print(f'Time taken: {time.time() - start} sec')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
