{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BigData 03.2 Spark Demo Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findspark/pyspark**\n",
    "As we are using findspark, find and start spark / pyspark first. This would not bee needed with a Pyspark kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() # find spark home directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/20 09:24:11 WARN Utils: Your hostname, hung.local resolves to a loopback address: 127.0.0.1; using 10.172.86.95 instead (on interface en0)\n",
      "23/10/20 09:24:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/20 09:24:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# create spark context object, spark context is the main entry point for spark functionality\n",
    "sc = pyspark.SparkContext(appName=\"HKABigData\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRange = range (1, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將Python範圍轉換為Spark Resilient Distributed Dataset (RDD)，RDD是Spark中的分布式數據結構\n",
    "# 他可以把資料轉移到cluster上面\n",
    "myRDD = sc.parallelize(myRange) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(myRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/20 09:24:23 ERROR Executor: Exception in task 7.0 in stage 0.0 (TID 7) 10]\n",
      "org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/10/20 09:24:23 WARN TaskSetManager: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "23/10/20 09:24:23 ERROR TaskSetManager: Task 7 in stage 0.0 failed 1 times; aborting job\n",
      "[Stage 0:>                                                         (0 + 9) / 10]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  1231975525 (0x496e7465)\nPython command to execute the daemon was:\n  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  1231975525 (0x496e7465)\nPython command to execute the daemon was:\n  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germany/HKA/02_BigData/Lab/Code/BigData-03-2-Spark.ipynb 儲存格 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shannon/Library/CloudStorage/OneDrive-%E5%9C%8B%E7%AB%8B%E8%87%BA%E7%81%A3%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8/NTUST/Germany/HKA/02_BigData/Lab/Code/BigData-03-2-Spark.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m myRDD\u001b[39m.\u001b[39;49mcollect()\n",
      "File \u001b[0;32m/usr/local/spark-3.5.0-bin-hadoop3/python/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m   1834\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  1231975525 (0x496e7465)\nPython command to execute the daemon was:\n  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: \nBad data in pyspark.daemon's standard output. Invalid port number:\n  1231975525 (0x496e7465)\nPython command to execute the daemon was:\n  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\nCheck that you don't have any unexpected modules or libraries in\nyour PYTHONPATH:\n  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\nAlso, check if you have a sitecustomize.py module in your python path,\nor in your python installation, that is printing to standard output\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/20 09:24:34 WARN TaskSetManager: Lost task 4.0 in stage 0.0 (TID 4) (10.172.86.95 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/20 09:24:45 WARN TaskSetManager: Lost task 5.0 in stage 0.0 (TID 5) (10.172.86.95 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/20 09:24:56 WARN TaskSetManager: Lost task 8.0 in stage 0.0 (TID 8) (10.172.86.95 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/20 09:25:07 WARN TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2) (10.172.86.95 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/20 09:25:18 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (10.172.86.95 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/20 09:25:29 WARN TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3) (10.172.86.95 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/20 09:25:40 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) (10.172.86.95 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/20 09:25:51 WARN TaskSetManager: Lost task 9.0 in stage 0.0 (TID 9) (10.172.86.95 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/20 09:26:02 WARN TaskSetManager: Lost task 6.0 in stage 0.0 (TID 6) (10.172.86.95 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 7 in stage 0.0 failed 1 times, most recent failure: Lost task 7.0 in stage 0.0 (TID 7) (10.172.86.95 executor driver): org.apache.spark.SparkException: \n",
      "Bad data in pyspark.daemon's standard output. Invalid port number:\n",
      "  1231975525 (0x496e7465)\n",
      "Python command to execute the daemon was:\n",
      "  /Users/shannon/anaconda3/bin/python -m pyspark.daemon\n",
      "Check that you don't have any unexpected modules or libraries in\n",
      "your PYTHONPATH:\n",
      "  /usr/local/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/usr/local/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "Also, check if you have a sitecustomize.py module in your python path,\n",
      "or in your python installation, that is printing to standard output\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:267)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    }
   ],
   "source": [
    "myRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這行程式碼創建了一個新的RDD，其中僅包含原始RDD中的偶數。\n",
    "myFilteredRDD = myRDD.filter(lambda x : x % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 這一行觸發了操作，將RDD的內容收集到本地，以列表的形式返回偶數數據。\n",
    "myFilteredRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "#Sample file from https://github.com/databricks/Spark-The-Definitive-Guide\n",
    "#file:///C:/users/jan/data/Spark-The-Definitive-Guide/Readme.md\n",
    "datasetpath=os.path.join('../Spark-The-Definitive-Guide/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取文本文件的內容，並將其轉換為一個包含行的RDD。\n",
    "myFileRDD = sc.textFile(os.path.join(datasetpath, 'Readme.md'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(myFileRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Spark: The Definitive Guide',\n",
       " '',\n",
       " 'This is the central repository for all materials related to [Spark: The Definitive Guide](http://shop.oreilly.com/product/0636920034957.do) by Bill Chambers and Matei Zaharia. ',\n",
       " '',\n",
       " '*This repository is currently a work in progress and new material will be added over time.*',\n",
       " '',\n",
       " '![Spark: The Definitive Guide](https://images-na.ssl-images-amazon.com/images/I/51z7TzI-Y3L._SX379_BO1,204,203,200_.jpg)',\n",
       " '',\n",
       " '# Code from the book',\n",
       " '',\n",
       " 'You can find the code from the book in the `code` subfolder where it is broken down by language and chapter.',\n",
       " '',\n",
       " '# How to run the code',\n",
       " '',\n",
       " '## Run on your local machine',\n",
       " '',\n",
       " 'To run the example on your local machine, either pull all data in the `data` subfolder to `/data` on your computer or specify the path to that particular dataset on your local machine.',\n",
       " '',\n",
       " '## Run on Databricks',\n",
       " '',\n",
       " \"To run these modules on Databricks, you're going to need to do two things.\",\n",
       " '',\n",
       " '1. Sign up for an account. You can do that [here](https://databricks.com/try-databricks).',\n",
       " '2. Import individual Notebooks to run on the platform',\n",
       " '',\n",
       " 'Databricks is a zero-management cloud platform that provides:',\n",
       " '',\n",
       " '- Fully managed Spark clusters',\n",
       " '- An interactive workspace for exploration and visualization',\n",
       " '- A production pipeline scheduler',\n",
       " '- A platform for powering your favorite Spark-based applications',\n",
       " '',\n",
       " '### Instructions for importing',\n",
       " '',\n",
       " '1. Navigate to the notebook you would like to import',\n",
       " '',\n",
       " \"For instance, you might go to [this page](https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/code/A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset.py). Once you do that, you're going to need to navigate to the **RAW** version of the file and save that to your Desktop. You can do that by clicking the **Raw** button. *Alternatively, you could just clone the entire repository to your local desktop and navigate to the file on your computer*.\",\n",
       " '',\n",
       " '2. Upload that to Databricks',\n",
       " '',\n",
       " 'Read [the instructions](https://docs.databricks.com/user-guide/notebooks/index.html#import-a-notebook) here. Simply open the Databricks workspace and go to import in a given directory. From there, navigate to the file on your computer to upload it. *Unfortunately due to a recent security upgrade, notebooks cannot be imported from external URLs. Therefore you must upload it from your computer*.',\n",
       " '',\n",
       " \"3. You're almost ready to go!\",\n",
       " '',\n",
       " \"Now you just need to simply run the notebooks! All the examples run on Databricks Runtime 3.1 and above so just be sure to create a cluster with a version equal to or greater than that. Once you've created your cluster, attach the notebook.\",\n",
       " '',\n",
       " '4. Replacing the data path in each notebook',\n",
       " '',\n",
       " \"Rather than you having to upload all of the data yourself, you simply have to change the path in each chapter from `/data` to `/databricks-datasets/definitive-guide/data`. Once you've done that, all examples should run without issue. You can use find and replace to do this very efficiently.\",\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFileRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這一行計算每一行的字符數，並將其轉換為一個新的RDD放入myLineLengthsRDD中。\n",
    "myLineLengthsRDD = myFileRDD.map(lambda line: len(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29,\n",
       " 0,\n",
       " 176,\n",
       " 0,\n",
       " 91,\n",
       " 0,\n",
       " 120,\n",
       " 0,\n",
       " 20,\n",
       " 0,\n",
       " 108,\n",
       " 0,\n",
       " 21,\n",
       " 0,\n",
       " 28,\n",
       " 0,\n",
       " 184,\n",
       " 0,\n",
       " 20,\n",
       " 0,\n",
       " 74,\n",
       " 0,\n",
       " 89,\n",
       " 53,\n",
       " 0,\n",
       " 61,\n",
       " 0,\n",
       " 30,\n",
       " 60,\n",
       " 33,\n",
       " 64,\n",
       " 0,\n",
       " 30,\n",
       " 0,\n",
       " 52,\n",
       " 0,\n",
       " 476,\n",
       " 0,\n",
       " 28,\n",
       " 0,\n",
       " 396,\n",
       " 0,\n",
       " 29,\n",
       " 0,\n",
       " 240,\n",
       " 0,\n",
       " 43,\n",
       " 0,\n",
       " 291,\n",
       " 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把myLineLengthsRDD中的資料收集到本地端，以列表的形式返回。\n",
    "myLineLengthsRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用reduce操作計算RDD中所有行的字符數的總和。\n",
    "# 在 Spark 集群中計算的，而不是在本地端計算。\n",
    "# Spark是一個分散式計算框架，它將數據分成多個分片，並將這些分片分佈在集群的多個節點上進行計算。\n",
    "# 當你調用 reduce 操作時，Spark會將計算分發給不同的節點，每個節點計算部分數據，然後將部分結果返回給主節點，主節點再將這些部分結果合併，最終返回計算的結果。\n",
    "myLineLengthsCount = myLineLengthsRDD.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2846\n"
     ]
    }
   ],
   "source": [
    "print(myLineLengthsCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將每行的單詞拆分並平坦化，創建一個包含所有單詞的RDD\n",
    "myFileWords = myFileRDD.flatMap(lambda line : line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Spark:',\n",
       " 'The',\n",
       " 'Definitive',\n",
       " 'Guide',\n",
       " '',\n",
       " 'This',\n",
       " 'is',\n",
       " 'the',\n",
       " 'central',\n",
       " 'repository',\n",
       " 'for',\n",
       " 'all',\n",
       " 'materials',\n",
       " 'related',\n",
       " 'to',\n",
       " '[Spark:',\n",
       " 'The',\n",
       " 'Definitive',\n",
       " 'Guide](http://shop.oreilly.com/product/0636920034957.do)',\n",
       " 'by',\n",
       " 'Bill',\n",
       " 'Chambers',\n",
       " 'and',\n",
       " 'Matei',\n",
       " 'Zaharia.',\n",
       " '',\n",
       " '',\n",
       " '*This',\n",
       " 'repository',\n",
       " 'is',\n",
       " 'currently',\n",
       " 'a',\n",
       " 'work',\n",
       " 'in',\n",
       " 'progress',\n",
       " 'and',\n",
       " 'new',\n",
       " 'material',\n",
       " 'will',\n",
       " 'be',\n",
       " 'added',\n",
       " 'over',\n",
       " 'time.*',\n",
       " '',\n",
       " '![Spark:',\n",
       " 'The',\n",
       " 'Definitive',\n",
       " 'Guide](https://images-na.ssl-images-amazon.com/images/I/51z7TzI-Y3L._SX379_BO1,204,203,200_.jpg)',\n",
       " '',\n",
       " '#',\n",
       " 'Code',\n",
       " 'from',\n",
       " 'the',\n",
       " 'book',\n",
       " '',\n",
       " 'You',\n",
       " 'can',\n",
       " 'find',\n",
       " 'the',\n",
       " 'code',\n",
       " 'from',\n",
       " 'the',\n",
       " 'book',\n",
       " 'in',\n",
       " 'the',\n",
       " '`code`',\n",
       " 'subfolder',\n",
       " 'where',\n",
       " 'it',\n",
       " 'is',\n",
       " 'broken',\n",
       " 'down',\n",
       " 'by',\n",
       " 'language',\n",
       " 'and',\n",
       " 'chapter.',\n",
       " '',\n",
       " '#',\n",
       " 'How',\n",
       " 'to',\n",
       " 'run',\n",
       " 'the',\n",
       " 'code',\n",
       " '',\n",
       " '##',\n",
       " 'Run',\n",
       " 'on',\n",
       " 'your',\n",
       " 'local',\n",
       " 'machine',\n",
       " '',\n",
       " 'To',\n",
       " 'run',\n",
       " 'the',\n",
       " 'example',\n",
       " 'on',\n",
       " 'your',\n",
       " 'local',\n",
       " 'machine,']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFileWords.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這一行將每個單詞映射為一個鍵值對，其中鍵是單詞本身，值為1。\n",
    "# 他要數單字的數量\n",
    "myFileWordsKV = myFileWords.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1),\n",
       " ('Spark:', 1),\n",
       " ('The', 1),\n",
       " ('Definitive', 1),\n",
       " ('Guide', 1),\n",
       " ('', 1),\n",
       " ('This', 1),\n",
       " ('is', 1),\n",
       " ('the', 1),\n",
       " ('central', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFileWordsKV.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將鍵值對按鍵進行分組，創建一個包含鍵和相關值的分組RDD。\n",
    "myFileWordsGroupedByKey = myFileWordsKV.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', <pyspark.resultiterable.ResultIterable at 0x105e74ed0>),\n",
       " ('Spark:', <pyspark.resultiterable.ResultIterable at 0x10b4f4f10>),\n",
       " ('Definitive', <pyspark.resultiterable.ResultIterable at 0x10b4f5050>),\n",
       " ('', <pyspark.resultiterable.ResultIterable at 0x10b4dd010>),\n",
       " ('for', <pyspark.resultiterable.ResultIterable at 0x10b4f5150>),\n",
       " ('materials', <pyspark.resultiterable.ResultIterable at 0x10b4f5210>),\n",
       " ('related', <pyspark.resultiterable.ResultIterable at 0x10b4f5290>),\n",
       " ('to', <pyspark.resultiterable.ResultIterable at 0x10b4de290>),\n",
       " ('Guide](http://shop.oreilly.com/product/0636920034957.do)',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x10b4f5350>),\n",
       " ('by', <pyspark.resultiterable.ResultIterable at 0x10b4f53d0>),\n",
       " ('Bill', <pyspark.resultiterable.ResultIterable at 0x10b4f5490>),\n",
       " ('Chambers', <pyspark.resultiterable.ResultIterable at 0x10b4f5510>),\n",
       " ('and', <pyspark.resultiterable.ResultIterable at 0x10b4f5590>),\n",
       " ('Matei', <pyspark.resultiterable.ResultIterable at 0x10b4f5610>),\n",
       " ('Zaharia.', <pyspark.resultiterable.ResultIterable at 0x10b4f5690>),\n",
       " ('work', <pyspark.resultiterable.ResultIterable at 0x10b4f52d0>),\n",
       " ('new', <pyspark.resultiterable.ResultIterable at 0x10b4f57d0>),\n",
       " ('material', <pyspark.resultiterable.ResultIterable at 0x10b4f5850>),\n",
       " ('will', <pyspark.resultiterable.ResultIterable at 0x10b4f58d0>),\n",
       " ('added', <pyspark.resultiterable.ResultIterable at 0x10b4f5950>),\n",
       " ('Guide](https://images-na.ssl-images-amazon.com/images/I/51z7TzI-Y3L._SX379_BO1,204,203,200_.jpg)',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x10b4f5a10>),\n",
       " ('from', <pyspark.resultiterable.ResultIterable at 0x10b4f5b10>),\n",
       " ('find', <pyspark.resultiterable.ResultIterable at 0x10b4f5c50>),\n",
       " ('`code`', <pyspark.resultiterable.ResultIterable at 0x10b4f5d90>),\n",
       " ('where', <pyspark.resultiterable.ResultIterable at 0x10b4f5ed0>),\n",
       " ('it', <pyspark.resultiterable.ResultIterable at 0x10b4f5fd0>),\n",
       " ('language', <pyspark.resultiterable.ResultIterable at 0x10b4f6110>),\n",
       " ('chapter.', <pyspark.resultiterable.ResultIterable at 0x10b4f6210>),\n",
       " ('How', <pyspark.resultiterable.ResultIterable at 0x10b4f6310>),\n",
       " ('##', <pyspark.resultiterable.ResultIterable at 0x10b4f6410>),\n",
       " ('Run', <pyspark.resultiterable.ResultIterable at 0x10b4f6550>),\n",
       " ('machine', <pyspark.resultiterable.ResultIterable at 0x10b4f6650>),\n",
       " ('To', <pyspark.resultiterable.ResultIterable at 0x10b4f6750>),\n",
       " ('pull', <pyspark.resultiterable.ResultIterable at 0x10b4f6810>),\n",
       " ('`/data`', <pyspark.resultiterable.ResultIterable at 0x10b4f6890>),\n",
       " ('specify', <pyspark.resultiterable.ResultIterable at 0x10b4f6990>),\n",
       " ('path', <pyspark.resultiterable.ResultIterable at 0x10b4f6a50>),\n",
       " ('dataset', <pyspark.resultiterable.ResultIterable at 0x10b4f6b50>),\n",
       " ('Databricks', <pyspark.resultiterable.ResultIterable at 0x10b4f6c10>),\n",
       " ('these', <pyspark.resultiterable.ResultIterable at 0x10b4f6d10>),\n",
       " ('modules', <pyspark.resultiterable.ResultIterable at 0x10b4f6e50>),\n",
       " ('going', <pyspark.resultiterable.ResultIterable at 0x10b4f6f90>),\n",
       " ('1.', <pyspark.resultiterable.ResultIterable at 0x10b4f7110>),\n",
       " ('Sign', <pyspark.resultiterable.ResultIterable at 0x10b4f7290>),\n",
       " ('up', <pyspark.resultiterable.ResultIterable at 0x10b4f73d0>),\n",
       " ('an', <pyspark.resultiterable.ResultIterable at 0x10b4f7510>),\n",
       " ('account.', <pyspark.resultiterable.ResultIterable at 0x10b4f7650>),\n",
       " ('[here](https://databricks.com/try-databricks).',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x10b4f7750>),\n",
       " ('Import', <pyspark.resultiterable.ResultIterable at 0x10b4f7890>),\n",
       " ('platform', <pyspark.resultiterable.ResultIterable at 0x10b4f79d0>),\n",
       " ('cloud', <pyspark.resultiterable.ResultIterable at 0x10b4f7b50>),\n",
       " ('-', <pyspark.resultiterable.ResultIterable at 0x10b4f7c50>),\n",
       " ('Fully', <pyspark.resultiterable.ResultIterable at 0x10b4f7dd0>),\n",
       " ('interactive', <pyspark.resultiterable.ResultIterable at 0x10b4f7f10>),\n",
       " ('workspace', <pyspark.resultiterable.ResultIterable at 0x10b4fc090>),\n",
       " ('A', <pyspark.resultiterable.ResultIterable at 0x10b4fc1d0>),\n",
       " ('production', <pyspark.resultiterable.ResultIterable at 0x10b4fc350>),\n",
       " ('Spark-based', <pyspark.resultiterable.ResultIterable at 0x10b4fc490>),\n",
       " ('applications', <pyspark.resultiterable.ResultIterable at 0x10b4fc5d0>),\n",
       " ('###', <pyspark.resultiterable.ResultIterable at 0x10b4fc710>),\n",
       " ('Navigate', <pyspark.resultiterable.ResultIterable at 0x10b4fc850>),\n",
       " ('you', <pyspark.resultiterable.ResultIterable at 0x10b4fc990>),\n",
       " ('would', <pyspark.resultiterable.ResultIterable at 0x10b4fcb10>),\n",
       " ('like', <pyspark.resultiterable.ResultIterable at 0x10b4fcc10>),\n",
       " ('instance,', <pyspark.resultiterable.ResultIterable at 0x10b4fcd50>),\n",
       " ('might', <pyspark.resultiterable.ResultIterable at 0x10b4fce10>),\n",
       " ('navigate', <pyspark.resultiterable.ResultIterable at 0x10b4fced0>),\n",
       " ('**RAW**', <pyspark.resultiterable.ResultIterable at 0x10b4fcfd0>),\n",
       " ('version', <pyspark.resultiterable.ResultIterable at 0x10b4fd090>),\n",
       " ('of', <pyspark.resultiterable.ResultIterable at 0x10b4fd190>),\n",
       " ('file', <pyspark.resultiterable.ResultIterable at 0x10b4fd290>),\n",
       " ('clicking', <pyspark.resultiterable.ResultIterable at 0x10b4fd390>),\n",
       " ('*Alternatively,', <pyspark.resultiterable.ResultIterable at 0x10b4fd450>),\n",
       " ('could', <pyspark.resultiterable.ResultIterable at 0x10b4fd510>),\n",
       " ('just', <pyspark.resultiterable.ResultIterable at 0x10b4fd610>),\n",
       " ('entire', <pyspark.resultiterable.ResultIterable at 0x10b4fd750>),\n",
       " ('Read', <pyspark.resultiterable.ResultIterable at 0x10b4fd850>),\n",
       " ('instructions](https://docs.databricks.com/user-guide/notebooks/index.html#import-a-notebook)',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x10b4fd910>),\n",
       " ('Simply', <pyspark.resultiterable.ResultIterable at 0x10b4fda10>),\n",
       " ('directory.', <pyspark.resultiterable.ResultIterable at 0x10b4fdb10>),\n",
       " ('From', <pyspark.resultiterable.ResultIterable at 0x10b4fdc90>),\n",
       " ('upload', <pyspark.resultiterable.ResultIterable at 0x10b4fde10>),\n",
       " ('recent', <pyspark.resultiterable.ResultIterable at 0x10b4fdfd0>),\n",
       " ('upgrade,', <pyspark.resultiterable.ResultIterable at 0x10b4f4bd0>),\n",
       " ('notebooks', <pyspark.resultiterable.ResultIterable at 0x10b4f4c50>),\n",
       " ('imported', <pyspark.resultiterable.ResultIterable at 0x10b4f4490>),\n",
       " (\"You're\", <pyspark.resultiterable.ResultIterable at 0x10b4f4950>),\n",
       " ('almost', <pyspark.resultiterable.ResultIterable at 0x10b4f4890>),\n",
       " ('ready', <pyspark.resultiterable.ResultIterable at 0x10b4f4550>),\n",
       " ('go!', <pyspark.resultiterable.ResultIterable at 0x10b4f4310>),\n",
       " ('Now', <pyspark.resultiterable.ResultIterable at 0x10b4f4410>),\n",
       " ('All', <pyspark.resultiterable.ResultIterable at 0x105e93250>),\n",
       " ('examples', <pyspark.resultiterable.ResultIterable at 0x10b4f2c50>),\n",
       " ('Runtime', <pyspark.resultiterable.ResultIterable at 0x10b4f2d10>),\n",
       " ('so', <pyspark.resultiterable.ResultIterable at 0x10b4f0710>),\n",
       " ('with', <pyspark.resultiterable.ResultIterable at 0x10b4f2a50>),\n",
       " ('than', <pyspark.resultiterable.ResultIterable at 0x10b4f2a10>),\n",
       " ('that.', <pyspark.resultiterable.ResultIterable at 0x10b4f1310>),\n",
       " (\"you've\", <pyspark.resultiterable.ResultIterable at 0x10b4f2890>),\n",
       " ('notebook.', <pyspark.resultiterable.ResultIterable at 0x10b4f2810>)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFileWordsGroupedByKey.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFileWordCount = myFileWordsKV.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 3),\n",
       " ('Spark:', 1),\n",
       " ('Definitive', 3),\n",
       " ('', 24),\n",
       " ('for', 5),\n",
       " ('materials', 1),\n",
       " ('related', 1),\n",
       " ('to', 29),\n",
       " ('Guide](http://shop.oreilly.com/product/0636920034957.do)', 1),\n",
       " ('by', 3),\n",
       " ('Bill', 1),\n",
       " ('Chambers', 1),\n",
       " ('and', 9),\n",
       " ('Matei', 1),\n",
       " ('Zaharia.', 1),\n",
       " ('work', 1),\n",
       " ('new', 1),\n",
       " ('material', 1),\n",
       " ('will', 1),\n",
       " ('added', 1),\n",
       " ('Guide](https://images-na.ssl-images-amazon.com/images/I/51z7TzI-Y3L._SX379_BO1,204,203,200_.jpg)',\n",
       "  1),\n",
       " ('from', 5),\n",
       " ('find', 2),\n",
       " ('`code`', 1),\n",
       " ('where', 1),\n",
       " ('it', 2),\n",
       " ('language', 1),\n",
       " ('chapter.', 1),\n",
       " ('How', 1),\n",
       " ('##', 2),\n",
       " ('Run', 2),\n",
       " ('machine', 1),\n",
       " ('To', 2),\n",
       " ('pull', 1),\n",
       " ('`/data`', 2),\n",
       " ('specify', 1),\n",
       " ('path', 3),\n",
       " ('dataset', 1),\n",
       " ('Databricks', 5),\n",
       " ('these', 1),\n",
       " ('modules', 1),\n",
       " ('going', 2),\n",
       " ('1.', 2),\n",
       " ('Sign', 1),\n",
       " ('up', 1),\n",
       " ('an', 1),\n",
       " ('account.', 1),\n",
       " ('[here](https://databricks.com/try-databricks).', 1),\n",
       " ('Import', 1),\n",
       " ('platform', 3),\n",
       " ('cloud', 1),\n",
       " ('-', 4),\n",
       " ('Fully', 1),\n",
       " ('interactive', 1),\n",
       " ('workspace', 2),\n",
       " ('A', 2),\n",
       " ('production', 1),\n",
       " ('Spark-based', 1),\n",
       " ('applications', 1),\n",
       " ('###', 1),\n",
       " ('Navigate', 1),\n",
       " ('you', 8),\n",
       " ('would', 1),\n",
       " ('like', 1),\n",
       " ('instance,', 1),\n",
       " ('might', 1),\n",
       " ('navigate', 3),\n",
       " ('**RAW**', 1),\n",
       " ('version', 2),\n",
       " ('of', 2),\n",
       " ('file', 3),\n",
       " ('clicking', 1),\n",
       " ('*Alternatively,', 1),\n",
       " ('could', 1),\n",
       " ('just', 3),\n",
       " ('entire', 1),\n",
       " ('Read', 1),\n",
       " ('instructions](https://docs.databricks.com/user-guide/notebooks/index.html#import-a-notebook)',\n",
       "  1),\n",
       " ('Simply', 1),\n",
       " ('directory.', 1),\n",
       " ('From', 1),\n",
       " ('upload', 3),\n",
       " ('recent', 1),\n",
       " ('upgrade,', 1),\n",
       " ('notebooks', 1),\n",
       " ('imported', 1),\n",
       " (\"You're\", 1),\n",
       " ('almost', 1),\n",
       " ('ready', 1),\n",
       " ('go!', 1),\n",
       " ('Now', 1),\n",
       " ('All', 1),\n",
       " ('examples', 2),\n",
       " ('Runtime', 1),\n",
       " ('so', 1),\n",
       " ('with', 1),\n",
       " ('than', 2),\n",
       " ('that.', 1),\n",
       " (\"you've\", 2),\n",
       " ('notebook.', 1),\n",
       " ('Replacing', 1),\n",
       " ('have', 1),\n",
       " ('chapter', 1),\n",
       " ('done', 1),\n",
       " ('should', 1),\n",
       " ('without', 1),\n",
       " ('issue.', 1),\n",
       " ('use', 1),\n",
       " ('this', 1),\n",
       " ('efficiently.', 1),\n",
       " ('The', 3),\n",
       " ('Guide', 1),\n",
       " ('This', 1),\n",
       " ('is', 4),\n",
       " ('the', 24),\n",
       " ('central', 1),\n",
       " ('repository', 3),\n",
       " ('all', 4),\n",
       " ('[Spark:', 1),\n",
       " ('*This', 1),\n",
       " ('currently', 1),\n",
       " ('a', 6),\n",
       " ('in', 6),\n",
       " ('progress', 1),\n",
       " ('be', 3),\n",
       " ('over', 1),\n",
       " ('time.*', 1),\n",
       " ('![Spark:', 1),\n",
       " ('Code', 1),\n",
       " ('book', 2),\n",
       " ('You', 4),\n",
       " ('can', 4),\n",
       " ('code', 2),\n",
       " ('subfolder', 2),\n",
       " ('broken', 1),\n",
       " ('down', 1),\n",
       " ('run', 7),\n",
       " ('on', 10),\n",
       " ('your', 11),\n",
       " ('local', 4),\n",
       " ('example', 1),\n",
       " ('machine,', 1),\n",
       " ('either', 1),\n",
       " ('data', 3),\n",
       " ('`data`', 1),\n",
       " ('computer', 2),\n",
       " ('or', 2),\n",
       " ('that', 6),\n",
       " ('particular', 1),\n",
       " ('machine.', 1),\n",
       " ('Databricks,', 1),\n",
       " (\"you're\", 2),\n",
       " ('need', 3),\n",
       " ('do', 5),\n",
       " ('two', 1),\n",
       " ('things.', 1),\n",
       " ('2.', 2),\n",
       " ('individual', 1),\n",
       " ('Notebooks', 1),\n",
       " ('zero-management', 1),\n",
       " ('provides:', 1),\n",
       " ('managed', 1),\n",
       " ('Spark', 1),\n",
       " ('clusters', 1),\n",
       " ('An', 1),\n",
       " ('exploration', 1),\n",
       " ('visualization', 1),\n",
       " ('pipeline', 1),\n",
       " ('scheduler', 1),\n",
       " ('powering', 1),\n",
       " ('favorite', 1),\n",
       " ('Instructions', 1),\n",
       " ('importing', 1),\n",
       " ('notebook', 2),\n",
       " ('import', 2),\n",
       " ('For', 1),\n",
       " ('go', 2),\n",
       " ('[this', 1),\n",
       " ('page](https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/code/A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset.py).',\n",
       "  1),\n",
       " ('Once', 3),\n",
       " ('that,', 2),\n",
       " ('save', 1),\n",
       " ('Desktop.', 1),\n",
       " ('**Raw**', 1),\n",
       " ('button.', 1),\n",
       " ('clone', 1),\n",
       " ('desktop', 1),\n",
       " ('computer*.', 2),\n",
       " ('Upload', 1),\n",
       " ('[the', 1),\n",
       " ('here.', 1),\n",
       " ('open', 1),\n",
       " ('given', 1),\n",
       " ('there,', 1),\n",
       " ('it.', 1),\n",
       " ('*Unfortunately', 1),\n",
       " ('due', 1),\n",
       " ('security', 1),\n",
       " ('cannot', 1),\n",
       " ('external', 1),\n",
       " ('URLs.', 1),\n",
       " ('Therefore', 1),\n",
       " ('must', 1),\n",
       " ('3.', 1),\n",
       " ('simply', 2),\n",
       " ('notebooks!', 1),\n",
       " ('3.1', 1),\n",
       " ('above', 1),\n",
       " ('sure', 1),\n",
       " ('create', 1),\n",
       " ('cluster', 1),\n",
       " ('equal', 1),\n",
       " ('greater', 1),\n",
       " ('created', 1),\n",
       " ('cluster,', 1),\n",
       " ('attach', 1),\n",
       " ('4.', 1),\n",
       " ('each', 2),\n",
       " ('Rather', 1),\n",
       " ('having', 1),\n",
       " ('yourself,', 1),\n",
       " ('change', 1),\n",
       " ('`/databricks-datasets/definitive-guide/data`.', 1),\n",
       " ('replace', 1),\n",
       " ('very', 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFileWordCount.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
