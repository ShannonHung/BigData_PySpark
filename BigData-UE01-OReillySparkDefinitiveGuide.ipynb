{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of this Tutorial are taken from the Spark Tutorial: Learning Apache Spark, and from O'Reillys book Spark the Definitive Guide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findspark/pyspark**\n",
    "As we are using findspark, find and start spark / pyspark first. This would not bee needed with a Pyspark kernel, where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:44:48.554427600Z",
     "start_time": "2023-10-18T17:44:48.549995100Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Spark Context**\n",
    "\n",
    "In Spark, communication occurs between a driver and executors. The driver has Spark jobs that it needs to run and these jobs are split into tasks that are submitted to the executors for completion. The results from these tasks are delivered back to the driver.\n",
    "\n",
    "\n",
    "In part 1, we saw that normal python code can be executed via cells. When using Databricks this code gets executed in the Spark driver's Java Virtual Machine (JVM) and not in an executor's JVM, and when using an IPython notebook it is executed within the kernel associated with the notebook. Since no Spark functionality is actually being used, no tasks are launched on the executors.\n",
    "\n",
    "In order to use Spark and its API we will need to use a SparkContext. When running Spark, you start a new Spark application by creating a SparkContext. When the SparkContext is created, it asks the master for some cores to use to do work. The master sets these cores aside just for you; they won't be used for other applications. When using a pyspark kernel or pyspark from the command line, the SparkContext is created for you automatically as sc.\n",
    "\n",
    "通过创建SparkContext来启动一个新的Spark应用程序。当创建SparkContext时，它会向主节点请求一些核心（CPU核心）来执行工作。主节点会为您保留这些核心，它们不会被其他应用程序使用。在使用pyspark内核或从命令行使用pyspark时，SparkContext会自动为您创建，并通常命名为\"sc\"。SparkContext是与Spark应用程序交互的关键接口，它允许您配置应用程序以及启动Spark作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:44:57.976581600Z",
     "start_time": "2023-10-18T17:44:50.758570700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/27 08:32:10 WARN Utils: Your hostname, hung.local resolves to a loopback address: 127.0.0.1; using 10.172.74.188 instead (on interface en0)\n",
      "23/10/27 08:32:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/27 08:32:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# creation is not needed with pyspark kernel, but as we're running locally:\n",
    "sc = pyspark.SparkContext(appName=\"ExcOReillySparkDefinitiveGuide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:44:57.992093900Z",
     "start_time": "2023-10-18T17:44:57.977594500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark Session object**\n",
    "\n",
    "When running in a pyspark console or with a pyspark kernel, the spark object has already been created.\n",
    "As we're using findspark, let's create a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:45:05.784996900Z",
     "start_time": "2023-10-18T17:45:05.754281700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x1198e2610>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook and Python Environment**\n",
    "\n",
    "A notebook is comprised of a linear sequence of cells. These cells can contain either markdown or code, but we won't mix both in one cell. When a markdown cell is executed it renders formatted text, images, and links just like HTML in a normal webpage. The text you are reading right now is part of a markdown cell. Python code cells allow you to execute arbitrary Python commands just like in any Python shell. Place your cursor inside the cell below, and press \"Shift\" + \"Enter\" to execute the code and advance to the next cell. You can also press \"Ctrl\" + \"Enter\" to execute the code and remain in the cell. These commands work the same in both markdown and code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:45:14.090752500Z",
     "start_time": "2023-10-18T17:45:14.083694100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 (main, Jul 25 2023, 17:36:13) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n"
     ]
    }
   ],
   "source": [
    "#Let's find out the python version\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:45:18.049414800Z",
     "start_time": "2023-10-18T17:45:18.040134400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 1 and 1 is 2\n"
     ]
    }
   ],
   "source": [
    "# This is a Python cell. You can run normal Python code here...\n",
    "# Let's first get the python version\n",
    "print('The sum of 1 and 1 is {0}'.format(1+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:45:19.864128500Z",
     "start_time": "2023-10-18T17:45:19.855225300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 1 and 2 is 3\n"
     ]
    }
   ],
   "source": [
    "# Here is another Python cell, this time with a variable (x) declaration and an if statement:\n",
    "x = 42\n",
    "if x > 40:\n",
    "    print ('The sum of 1 and 2 is {0}'.format(1+2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook state**\n",
    "\n",
    "As you work through a notebook it is important that you run all of the code cells.  The notebook is stateful, which means that variables and their values are retained until the notebook is detached (in Databricks) or the kernel is restarted (in IPython notebooks).  If you do not run all of the code cells as you proceed through the notebook, your variables will not be properly initialized and later code might fail.  You will also need to rerun any cells that you have modified in order for the changes to be available to other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:45:30.529658400Z",
     "start_time": "2023-10-18T17:45:30.524307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "# This cell relies on x being defined already.\n",
    "# If we didn't run the cells from part above this code would fail.\n",
    "print (x * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Library imports**\n",
    "\n",
    "We can import standard Python libraries (modules) the usual way. An import statement will import the specified module. In this tutorial and future labs, we will provide any imports that are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:45:33.777100600Z",
     "start_time": "2023-10-18T17:45:33.763991500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was last run on: 2023-10-27 08:32:11.188872\n"
     ]
    }
   ],
   "source": [
    "# Import the datetime library\n",
    "import datetime\n",
    "print('This was last run on: {0}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Cluster**\n",
    "\n",
    "\n",
    "The diagram below shows an example cluster, where the cores allocated for an application are outlined in purple. (Note: *In the case of the Community Edition tier there is no Worker, and the Master, not shown in the figure, executes the entire code.*)\n",
    "\n",
    "![executors](http://spark-mooc.github.io/web-assets/images/executors.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can view the details of your Spark application in the Spark web UI.  The web UI is typically accessible through a cluster UI.  When running locally you'll find it at [localhost:4040](http://localhost:4040) (if localhost doesn't work, try [this](http://127.0.0.1:4040/)).  In the web UI, under the \"Jobs\" tab, you can see a list of jobs that have been scheduled or run.  It's likely there isn't any thing interesting here yet because we haven't run any jobs, but we'll return to this page later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O'Reilly Data Set**\n",
    "\n",
    "\n",
    "For starters, we will be using the data set from \"Spark - The Definitive Guide\" by Bill Chambers & Matei Zaharia, published by O'Reilly.\n",
    "\n",
    "In particular, start with reading page 22ff\n",
    "\n",
    "Find the data set on [GitHub](https://github.com/databricks/Spark-The-Definitive-Guide), download it, and put it to a known folder location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:45:40.037815500Z",
     "start_time": "2023-10-18T17:45:40.021038400Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#file:///C:/users/jan/data/Spark-The-Definitive-Guide/data/flight-data/csv/\n",
    "datasetpath=os.path.join('../Spark-The-Definitive-Guide')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flight Data Example Book Page 50ff**\n",
    "- 以下的spark是一个SparkSession对象，它是与Spark集群通信的入口点。\n",
    "- read是用于读取数据的方法。\n",
    "- option(\"inferSchema\", \"true\")和option(\"header\", \"true\")分别用于自动推断数据的模式（数据类型）以及将第一行视为列标题。\n",
    "- csv(...)函数用于加载CSV文件，文件的路径由glob.glob(...)动态生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:45:50.393635600Z",
     "start_time": "2023-10-18T17:45:42.439734800Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "flightData2015 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\\\n",
    ".csv(glob.glob(os.path.join(datasetpath, 'data', 'flight-data','csv', '2010-summary.csv')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:45:54.538274100Z",
     "start_time": "2023-10-18T17:45:54.502026800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T19:09:07.222599400Z",
     "start_time": "2023-10-08T19:09:05.829480700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:45:58.022800Z",
     "start_time": "2023-10-18T17:45:57.745042200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=264),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=69)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:45:59.577248Z",
     "start_time": "2023-10-18T17:45:59.420912200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#19 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=63]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:01.269050700Z",
     "start_time": "2023-10-18T17:46:00.985337800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='Equatorial Guinea', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:03.218142100Z",
     "start_time": "2023-10-18T17:46:03.198581900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flight Data Example SQL Page 53ff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:07.864943300Z",
     "start_time": "2023-10-18T17:46:07.804576600Z"
    }
   },
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:08.899956400Z",
     "start_time": "2023-10-18T17:46:08.744726400Z"
    }
   },
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, count(1) FROM flight_data_2015 GROUP BY DEST_COUNTRY_NAME\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:09.823129400Z",
     "start_time": "2023-10-18T17:46:09.659305300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 200), ENSURE_REQUIREMENTS, [plan_id=85]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:10.806123300Z",
     "start_time": "2023-10-18T17:46:10.761670300Z"
    }
   },
   "outputs": [],
   "source": [
    "dataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:11.629619900Z",
     "start_time": "2023-10-18T17:46:11.573144300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 200), ENSURE_REQUIREMENTS, [plan_id=98]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/shannon/Library/CloudStorage/OneDrive-國立臺灣科技大學/NTUST/Germa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以直接下 sql \n",
    "- 或是透過select()方法來選擇欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:13.425011200Z",
     "start_time": "2023-10-18T17:46:12.492699100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=348113)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max(count) 是一个聚合函数，它返回一个列的最大值\n",
    "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:13.655978300Z",
     "start_time": "2023-10-18T17:46:13.428535500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=348113)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 5 by country in SQL and DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:17.204380500Z",
     "start_time": "2023-10-18T17:46:16.036394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           384932|\n",
      "|           Canada|             8271|\n",
      "|           Mexico|             6200|\n",
      "|   United Kingdom|             1629|\n",
      "|          Germany|             1392|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:17.759084900Z",
     "start_time": "2023-10-18T17:46:17.292327800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           384932|\n",
      "|           Canada|             8271|\n",
      "|           Mexico|             6200|\n",
      "|   United Kingdom|             1629|\n",
      "|          Germany|             1392|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import desc\n",
    "flightData2015\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\")\\\n",
    ".withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\"))\\\n",
    ".limit(5)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T17:46:19.583743600Z",
     "start_time": "2023-10-18T17:46:18.859769900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+-----+\n",
      "|   DEST_COUNTRY_NAME|Mean Count|StdDev|Count|\n",
      "+--------------------+----------+------+-----+\n",
      "|            Anguilla|      21.0|  NULL|    1|\n",
      "|              Russia|     152.0|  NULL|    1|\n",
      "|            Paraguay|      90.0|  NULL|    1|\n",
      "|             Senegal|      29.0|  NULL|    1|\n",
      "|              Sweden|      65.0|  NULL|    1|\n",
      "|            Kiribati|      17.0|  NULL|    1|\n",
      "|              Guyana|      17.0|  NULL|    1|\n",
      "|         Philippines|     132.0|  NULL|    1|\n",
      "|            Malaysia|       1.0|  NULL|    1|\n",
      "|           Singapore|      25.0|  NULL|    1|\n",
      "|                Fiji|      53.0|  NULL|    1|\n",
      "|              Turkey|      75.0|  NULL|    1|\n",
      "|             Germany|    1392.0|  NULL|    1|\n",
      "|         Afghanistan|      11.0|  NULL|    1|\n",
      "|              Jordan|      50.0|  NULL|    1|\n",
      "|               Palau|      31.0|  NULL|    1|\n",
      "|Turks and Caicos ...|     136.0|  NULL|    1|\n",
      "|              France|     774.0|  NULL|    1|\n",
      "|              Greece|      50.0|  NULL|    1|\n",
      "|              Taiwan|     275.0|  NULL|    1|\n",
      "+--------------------+----------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import desc\n",
    "import pyspark.sql.functions as func\n",
    "flightData2015\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".agg(\\\n",
    "    func.mean(\"count\").alias(\"Mean Count\"),\n",
    "    func.stddev(\"count\").alias(\"StdDev\"),\n",
    "     func.count(\"count\").alias(\"Count\")\\\n",
    "    )\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uebung 2 - Exercise**\n",
    "\n",
    "Find a bunch of python / spark examples where there will be parallelism in data processing. Look into the Spark UI to find out how and how paralell execution is organized in Spark. Is there an operation comparable to the \"reduce\" part of Mapreduce?\n",
    "Hint: find out more about Spark's execution model under http://spark.apache.org/docs/latest/cluster-overview.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
