{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import findspark \n",
    "findspark.init()\n",
    "\n",
    "# for sql\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import col, max as spark_max, count, sum as spark_sum, datediff, lit, min as spark_min\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# for time \n",
    "import time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1, ZIP=76133, AGE=20, SIZE=160.0),\n",
       " Row(ID=2, ZIP=7131, AGE=20, SIZE=170.0),\n",
       " Row(ID=3, ZIP=76133, AGE=40, SIZE=175.0),\n",
       " Row(ID=4, ZIP=76133, AGE=40, SIZE=175.0),\n",
       " Row(ID=5, ZIP=76189, AGE=30, SIZE=180.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建立Spark配置\n",
    "spark = SparkSession.builder.appName('finalExam').getOrCreate()\n",
    "csv_path = \"../data/finalExam.csv\" \n",
    "rdd = spark.sparkContext.textFile(csv_path)\n",
    "\n",
    "rdd1 = spark.read\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(csv_path)\n",
    "\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-----+\n",
      "| ID|  ZIP|AGE| SIZE|\n",
      "+---+-----+---+-----+\n",
      "|  3|76133| 40|175.0|\n",
      "|  4|76133| 40|175.0|\n",
      "|  5|76189| 30|180.0|\n",
      "+---+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1 = rdd1.filter(col(\"AGE\") >= 30)\n",
    "rdd1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ZIP=76133), Row(ZIP=7131), Row(ZIP=76133), Row(ZIP=76133), Row(ZIP=76189)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get distinct values for the selected column\n",
    "selected_column = \"SIZE\"\n",
    "rdd2 = rdd1.select(col(selected_column))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTINCT(ZIP) =\n",
      "+-----+\n",
      "|ZIP  |\n",
      "+-----+\n",
      "|7131 |\n",
      "|76133|\n",
      "|76189|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rdd3 = rdd2.distinct()\n",
    "# Show the result\n",
    "print(f\"DISTINCT({selected_column}) =\")\n",
    "\n",
    "rdd3.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
